# 多视角学习

联合训练式算法交替训练，以最大限度地实现两种不同数据视图的相互一致性；

多核学习算法利用自然对应于不同视图的核，并线性或非线性地组合核以提高学习性能；

子空间学习算法的目标是通过假设输入视图是从该潜在子空间生成的，从而获得由多个视图共享的潜在子空间

## 多视角学习的原则

### 共识原则

要达成多个视角之间的一致性
$$
P(f^1 \neq f^2) \geq max \{P_{err}(f_1),P_{err}(f_2)\}
$$
两个视角不一致的概率是两者错误率最大者的上界，所以要最小化不一致的概率，就可以减小错误

最小化标签数据之间的错误以及最大化无标签之间的一致性

**联合正则化**算法中，一致性原则可以表示为:
$$
min \sum_{i \in U}[f^1(x_i)-f^2(x_i)]^2 + \sum_{i\in L}V(y_i,f(x_i))\\
第一部分强调了两个在无标签数据的不同视角的一致性，第二个部分评估损失函数在经验数据\\
上的经验损失
$$
支持向量机可以被认为是将特征投影到一维空间，然后进行阈值处理，在此之后，支持向量机-2K强制在该一维空间上约束两个视图的一致性。在形式上，该约束可以写成
$$
||f^1(x_i^1)-f^2(x^2_i)||\leq \eta_i + \epsilon\\
\eta是一个推进两个视角一致性的变量，\epsilon是平滑变量
$$

### 互补原则

当一个样本被一个分类器以一个视角打上标签后，第二个分类器想在第二个视角进行有用的分类的话，就得保证h1所掌握的信息是h2没有的



一种传统的算法是合并不同视角的向量形成一个新的向量，然后使用单视角学习的算法。但这种合并会导致在小样本上过拟合，以及每个视角上的特征都会被忽略。

## 如何去构造多视角以及如何评估视角

**三类：**

​	-通过随机方法从元数据中构建多视角

​	-原始单视图特征重塑或分解为多个视图的算法，如矩阵表达或多核

​	-自动划分特征集

### 视角构建

生成不同的视角对应于特征划分，这包含了特征选择

特征集划分将原始特征集分解为多个不相交的子集来构造每个视图，而不是提供单个代表性的特征集

一种方法是，随机分解原始的特征集，但这样没有保证

RSM（随机子空间法）：该方法依赖于一个自主的伪随机过程，从给定的特征空间中选择少量维度。通过在每个过程中给未选择维度中的所有点一个常数值（零），进行选择并固定子空间。对于给定的n维特征空间，可以构造2^n个这样的选择。所有子空间都可以被视为数据的不同视图。虽然大多数其他方法都受到维数灾难的影响，但该方法利用了高维数



#### 聚类：这些方法涉及基于相似性度量的特征聚合，目的是促进视图之间的多样性。

#### 随机选择：与特征空间装袋相结合，随机选择可以从光谱特征空间中获得更大的信息探索，并可以消除生成无信息或损坏视图的影响。

#### 均匀带切片：在整个光谱范围内对数据进行均匀分割，创建包含以相等间隔分隔的带的视图，从而保证视图的充分性。

#### 文章特征分解算法--PMC（伪多视角协同训练）

对于一个线性表达：f（x）=wx+b，给定了权重向量w，则要对进行优化：
$$
min_{w_1,w_2}log(e^{L(w_1;L)}+e^{L(w_1;L)})\\
w_1,w_2是每个分类器的权重向量\\
L（w;L）是L数据集的log损失
$$
为了保证两个分类器是在同一个数据的不同视角进行训练的，对于每个特征i，至少其中之一的分类器必须拥有i维的0权重这个约束
$$
\forall i,1\leq i \leq d,w_1^iw_2^i=0
$$
解决上述的优化问题，就可以自动的找到一个优的特征划分

### 视角评估

#### 视图验证算法

用于预测视图能否足够兼容以解决多视图学习任务

尝试用监督的方法去学习一个决策树，根据视图是否兼容多视角学习来区分不同任务

设计了一组特征来指示视图的不兼容程度，并通过在测试集上比较单视图和多视图算法的准确性来自动生成每个实例的标签

#### 半监督交叉特征学习

减轻当视角对于学习概念本身是不充分时候的协同训练出现的问题。

使用两个视角的初始分类器对无标签数据打标签之后，单独基于未标记数据训练每个视图的两个单独分类器以消除此问题。在验证数据V的帮助下，可以对所有四个分类器进行加权组合，以检测在不影响初始分类器性能的情况下从未标记数据中可以获得多少益处。如果来自未标记数据的预测太过嘈杂而无法使用，则可以简单地将在未标记数据上新学习的两个分类器的组合权重归零，然后我们返回到在标记数据上训练的初始分类器。

为了检测以及过滤视角分歧，定义条件视角熵H（xi|xj）作为给定观察视角xi下的不确定性测量，通过设定条件视角熵阈值，有分歧的视角上的样本会在每轮迭代中抛弃

#### 概率方法--贝叶斯协同训练

应用了对每一个视角用一个隐含的变量fj，以及一个共同隐含变量fc去对不同视角得一致性建模

最后\psi（fj，fc）代表了第j个视角和共识函数之间的兼容性
$$
\psi(f_j,f_c)=exp(-\frac{||f_j-f_c||^2}{2\sigma^2_j})\\
\sigma是可靠性指标以及控制第j个视角和共识隐变量之间的相互作用
$$
扩展到异方差的情况（即每个观察都会被不同等级的噪声破坏）
$$
\psi(f_j,f_c)=N(f_j,A_j)\\
A_j是噪声方差矩阵，假设是独立同分布的噪声，则噪声矩阵是\\
A_j = diag(\sigma^2_{1,j},...,\sigma^2_{N,j})\\
\sigma^2_{i,j}是在视角j上的噪声样本i的评估
$$
当多核学习中，有些核是无关或者噪声的，就需要对不同核进行赋权。

提出了一种贝叶斯局部方法，将不同的特征表示与高斯过程相结合，学习每个视图的局部权重

#### 置信度

内部视图（intra）置信度

视图间（inter）置信度
$$
设X有M个视角，则观测数据可以表示为X^1,...,X^M,根据互信息定义，\\
视图间置信度可以定义为：C_{inter}(X)=\sum^M_{i=1}\sum^M_{j=i}\frac{1}{I(X^i,X^j)}\\
I()是X^i和X^j之间互信息的度量，最大化inter置信度就可以减少全局的依赖性\\
此外，作者提出了以半监督方式迭代计算和最小化标记和未标记数据的总不一致性\\
C_{intra}(X)=\sum^M_{i=1}\frac{1}{F(X^L_i,X^U_i,S_i)}\\
X^L_i和X^U_i分别是标注和未标注数据，S_i是i视角下的相似度矩阵，F是度量一致性
$$

#### 典型相关分析（CCA）

用于描述两个视角之间的线性关系，以此去计算共享嵌入，使得两个视图之间的变量之间的相关性在嵌入空间中最大化。可以视作多视角的PCA

加上一个核，可以验证非线性关系
$$
对于两个视角X_{d×n}，Y_{k×n},CCA计算两个投影向量w_x,w_y,计算：\\
\rho = \frac{w_x^TXY^Tw_y}{\sqrt{(w^T_xXX^Tw_x)(w^T_yYY^Yw_y)}}\\
其中w_x=X\alpha,w_y=Y\beta,两个未知数是N维的向量
$$
文献中的几个关联度量被构造为相关系数的函数，其中两个最常见的关联度量如下。
$$
r(X,Y)=\rho_1\\
r(X,Y)=-\sum_{i=1}log(1-\rho_i^2)
$$

## 视角结合

#### 协同训练

每个视角训练一个只使用该视角下的特征的分类器。通过最大化在标注数据集上的两个分类器的预测的一致性，同时最小化在无标注数据集上的两个分类器的预测的不一致性，这些分类器互相学习并达到最优解。无标签数据被视作验证集。

每轮迭代中，一个视角上的学习器会对无标签数据进行标注，然后会将这些数据添加到另一个学习器的训练池中。这样两个视角下的信息可以交换。联合正则项可以被视作联合训练的正则化版本。不像联合训练，联合正则化算法使用上面的一致性原则。



如果验证集没有提供，比如在无监督学习立马，就需要在每个视角训练分类器同时验证同一训练集上的试图组合。

#### 多视角数据下的谱聚类算法

在假设真正的基础聚类将每个视图中的对应点分配给同一聚类的情况下，该算法求解单个图上的谱聚类，以获得每个视图中有区别的特征向量U1（U2），然后使用U1（U2）对点进行聚类，并使用该聚类分别修改视图2（1）中的图结构。



在多视图监督学习问题中，还使用隐式验证集来组合多个视图。

#### 贝叶斯协同训练

构造了高斯过程协同训练的贝叶斯无向图形模型

有一个隐函数fc去保证每个样本的输出y的条件独立性，以及每个视角的隐函数fj，

{fc}被视作隐式验证集，在隐空间连接多个视角。

最好的做法是使用一个数据集并允许一个算法去选择合适的核以及核的组合。





由于不同的核可能对应不同的相似性概念或来自不同表示的输入，可能来自多个源或模式，因此组合核是整合多个信息源并找到更好解决方案的一种可能方式

#### 核的结合方法：

**线性组合法**

-直接对核求和
$$
K(x_i,x_j)=\sum_{k=1}^MK_k(x_i,x_j)
$$
-权重对核求和
$$
K(x_i,x_j)=\sum_{k=1}^Md_kK_k(x_i,x_j)
$$
组合核从如下集合中选出
$$
K = \{K:K=\sum_{k=1}^Md_kK_k,K\geq0,tr(K)\leq c)\}\\
K = \{K:K=\sum_{k=1}^Md_kK_k,d_k\geq 0,K\geq 0,tr(K)\leq c\}
$$
根据数据分布来定义核函数的权重，并定义局部组合核的矩阵为：
$$
K(x_i,x_j)=\sum_{k=1}^Md_k(x_i)K_k(x_i,x_j)d_k(x_j)\\
d_k(x)是用作选择特征空间作为输入x的门控函数
$$
**非线性方法**

尝试使用基本核和其他产生正定核的组合的乘积来执行多核学习；

-组合核的幂和幂方法：
$$
K(x_i,x_j)=exp(-\sum_{k=1}^Md_kx_i^TA_kx_j)\\
或者是K(x_i,x_j)=(d_0+\sum_{k=1}^Md_kx_i^TA_lx_j)^n
$$
-基于核回归和多项式核组合的非线性核组合方法
$$
k=\sum_{0\leq k_1+...+k_M\leq d,k_m \geq 0} \mu_{k_1...k_M}\prod_{m=1}^MK_m(x_i,x_j)^{k_m},\mu_{k_1...l_M}\geq 0\\
目标就是寻找一个向量\mu=(\mu_1,...,\mu_M)^T
$$
基于子空间学习的方法旨在通过假设输入视图是从该潜在子空间生成的，从而获得由多个视图共享的潜在子空间

-以高斯过程为工具，构造一个潜变量模型，完成非线性降维任务

-还有研究了研究了跨模态检索任务中的度量学习问题

​	多视图数据度量学习的目标是学习可以将原始多视图高维特征投影到共享特征空间中的度量，因此该空间中的欧几里德距离不仅在单个视图中有意义，而且在不同视图之间也有意义



#### 结合维度的方法总结

(a)协同训练算法通常在不同的视角训练各自的学习器，然后强制在不同视角上保持一致。这种方法就是一种后期组合多视角，因为前期训练时各自独立的

(b)多核学习算法计算在不同视角各自的核，这种方法可以是做一个多视角的中期组合，因为核（视角）时在训练期间组合的

(c)基于子空间方法旨在通过假设输入数据时由隐视角生成的以此获得一个合适的子空间。可以视作多视角的前期组合，因为多视角是直接在一起考虑成一个共享子空间



## 协同训练算法

### 一些前提假设

协同训练考虑以一个集合，其所有的样本都可以被分成2个独立的视角并有三个主要的假设

(a)充分性：每个视角本身足以进行分类

(b)完备性：两个视图中的目标函数以高概率预测共同出现的特征的相同标签

(c)条件独立性：给定类标签，视角是条件独立的

​	当两个充分的视角在给定类标签下是条件独立的，协同训练才能成功

​	定理，如果在视角X1,2上个的概念类C1，2在PAC模型中可以学习的，尽管有分类噪声，如果有条件独立假设满足，那么（C1,C2）在仅从无标签数据中的协同训练模型（在给定一个初试弱有用的预测其h(x1)）就是可以学习的。

​	特别的，让分类噪声（\alpha,\beta）是数据集中真正样本以\alpha概率错误标记，真反样本以\beta概率错误标记。定义一个f(x)作为目标概念以及p = PrD(f(x)=1)作为从D中随机抽取样本为正的可能性。因此两个噪声率的和适用于
$$
\alpha + \beta \leq \frac{1-\epsilon^2}{(p(1-p))}
$$
​	具有PAC风格的对协同训练的界限
$$
让S是独立样本集，数据集X上的部分规则h是X到标签集的映射\{1,..,k,\bot\}\\
k是类标签的数量，\bot是表示部分规则h没有作用，对于所有规则h1和h2，根据S至少1-\delta的选择\\
得到结论：\\
如果\gamma_i(h_1,h_2,\delta/2) > 0其中1\leq i\leq k，然后f是排列\\
P(h_1=i|f(y)=i,h1\neq\bot)\leq\frac{1}{\gamma_i(h_1,h_2,\delta)}(\epsilon_i(h_1,h_2,\delta)+\hat P(h_1\neq i|h_2=i,h_1\neq \bot))\\
其中\epsilon_i(h_1,h_2,\delta)=\sqrt{\frac{ln2(|h_1|+|h_2|)+ln2/\delta}{2|S(h_2=i,h_1\neq\bot)|}}\\
\gamma_i(h_1,h_2,\delta)=\hat P(h_1=i|h_2=i,h_1\neq\bot)-\hat P(h_1\neq i|h_2=i,h_1\neq \bot) - 2\epsilon_i(h_1,h_2,\delta)
$$


-弱依赖假设

放松了条件独立假设，弱依赖也可以成功联合训练
$$
给定映射函数Y=y，定义对立视图规则h_1和h_2的条件依赖为：\\
d_y = \frac{1}{2}\sum_{u,v}|Pr[h_1=v|Y=y,h_2=u]-Pr[h_1=v|Y=y]|\\
如果h_1和h_2是条件独立的，那么d_y=0，h_1和h_2适用于弱规则依赖仅仅在如下情况：\\
d_y \leq p_2\frac{q_1-p_1}{2p_1q_1}\\
p_1 = min_uPr[h_2=u|Y=y],p_2=min_uPr[h_1=u|Y=y],q_1=1-p_1
$$
-扩张假设

在每个特征集上给出适当的强PAC学习算法就足以使得迭代协同训练成功

那么P r(S1∧S2)表示对两个视图都有信心的例子上的概率质量，而P r(S1⊕S2)表示只对一个视图有信心的例子上的概率质量
$$
设对于任意S_1 \in X_1^+以及S_2\in X_2^+\\
	X^+表示是X空间上的正域，X^-是负域\\
Pr(S_1\oplus S_2)\geq \epsilon min[Pr(S_1\wedge S_2),Pr(\bar S_1\wedge \bar S_2)]
$$
左右扩展
$$
Pr(S_1)\leq1/2,Pr(S_2|S_1)\geq 1-\epsilon\\
Pr(S_2)\geq(1+\epsilon)Pr(S_1)
$$
-大多样性假设

​	当两个学习器的差异性大于他们的误差，则效果可以提升

​	如果分类器hi标记的示例对分类器hj有用，那么hi应该知道一些hj不知道的信息。也就是说，hi和hj应该有显著差异。随着协同训练过程的进行，两个分类器之间的相似度会越来越高，它们之间的差异会越来越小，因为两个分类器会给彼此标记越来越多的未标记实例。

### 联合训练

给定一个标签数据集L，一个无标签数据集U，算法首先建立一个小的池U‘用于保存u无标签数据。然后根据一下步骤迭代：

-首先使用有标签数据集L分别在两个视角x1和x2训练两个朴素贝叶斯分类器h1和h2

-然后用这两个分类器去检测无标签数据集U’并且添加其最确信标记为正的p个样本和最确信标记为负的n个样本添加到L，以及相应分类器分配的标签

-最后，池U‘将被从U中随机渲染2p+2n个的样本填满

#### C0-EM

联合训练是指，分类器h1添加在已标注数据集上的样本，可以被h2拿去学习。

与传统联合训练相反，在每个视角上使用EM并给定无标注样本概率标签，概率标签可能会在每一次迭代中更改，通过以概率的方式重新构造支持向量机，并以概率估计未标记数据的标签。

#### Co-clustering

将常用的聚类算法应用到多视角上

Kmeans

​	在每轮迭代，在一个视角上允许k均值，然后将分区信息转换到另一个视角并在第二个视角运行k均值。停止后，计算每个集群和视图的共识均值，然后将每个样本分配给通过封闭概念向量确定的不同集群。

#### 基于图的联合训练

-通过高斯过程的贝叶斯无向图模型

​	假设有m个不同视角的n个样本{xi}有输出{yi}，fj表示第j个视角的隐函数，让fj~GP（0，k），隐函数fc是保证输出y以及m个视角隐函数fj之间的条件独立性。在功能层面，输出y仅仅依赖于fc，潜在函数fj仅仅通过共识函数fc相互依赖。，得到连接概率
$$
p(y,f_c,f_1,...,f_m)=\frac{1}{Z}\psi (y,f_c)\prod_{j=1}^m\psi(f_j,f_c)
$$
在具有n个样本的地面网络中
$$
让f_c = \{f_c(x_i)\}^n_{i=1}和f_j=\{f_j(x_i^j)\}^n_{i=1}\\
则图模型是如下形式；\\
p(y,f_c,f_1,...,f_m)=\frac{1}{Z}\psi (y,f_c(x_i))\prod_{j=1}^m\psi(f_j)\psi(f_j,f_c)\\
\psi(f_j)是描述了各个视角j之间的依赖结构，\psi(f_j,f_c)是描述了每个视角的隐函数如何于fc关联\\
结合高斯过程，可以得到这两个值：\\
\psi(f_j)=exp(-\frac{1}{2}f_j^TK_j^{-1}f_j),\psi(f_j,f_c)=exp(-\frac{||f_j-f_c||^2}{2\sigma^2_j})\\
我们可以得到联合训练的核：\\
K_c = [\sum_j(K_j+\sigma_j^2I)^{-1}]^{-1}
$$
联合训练的核揭示了来自不同视图的内核如何在多视图学习中组合，并允许我们简单地解决 GP 分类问题。

在一个视图中，标签可以从初始标记示例传播到未标记示例，并且这些新标记的示例可以添加到另一个视图中。然后，另一个视图可以将初始标记示例的标签和这些新标记示例的标签传播到剩余的未标记实例。

#### 多学习器算法

假设将样本空间分为一组等价类

假设 A 和 B 是两种不同的监督算法，U 是未标记数据，L 是原始标记数据，LA 是 B 为 A 标记的数据，LB 是 A 为 B 标记的数据。在每次迭代开始时，在标记示例 L 并LA 上训练 A 以获得假设 HA。同样，在 L 并 LB 上训练 B 以获得 HB。每个算法都考虑它的每个等价类，并决定使用哪个来标记来自 U 的数据以供其他算法使用。这种协同训练算法重复进行，直到在迭代期间 LA 和 LB 都没有变化。

-三学习器算法

Tri-training 从原始标记示例集中生成三个分类器，然后在迭代中使用未标记示例进行细化。对于每次迭代，如果其他两个分类器在特定条件下同意标记，则为分类器标记一个未标记的示例。

-多训练SVM（MTSVM）

最初，一系列特征子集 - 换句话说，可以使用随机子空间方法从原始输入特征中获得数据的多个视图。然后可以在这些生成的视图上学习多个分类器，并可以在半监督相关反馈设置中相互训练。最后，使用多数投票规则生成最优分类器。



## 多核学习

 MKL 中的内核自然对应于不同的视图，适当地组合内核可以提高学习性能。

### Boosting 方法

-多重加法回归内核

考虑由不同内核函数和参数形成的大型内核矩阵库。
$$
函数定义为：\\
f(x)=\sum_{i=1}^N\sum_{k=1}^Md_i^kK_k(x_i^m,x^m)+b\\
这是由异构核函数K_1,...,K_m的线性组合组成的
$$
与集成方法一样，内核的每一列都被视为一个假设，内核列是动态生成的

Column Generation技术可以用于解决大规模线性规划问题

沿着与Boosting算法类似的思路，可以使用这两个损失函数之一迭代更新组合核矩阵。
$$
ExpLoss(K(x_1,x_2),y_1,y_2) = exp(-y_1y_2K(x_1,x_2))\\
LogLoss(K(x_1,x_2),y_1,y_2) = log(1+exp(-y_1y_2K(x_1,x_2)))
$$

### 半正定规划

优化算法如下：
$$
目标是寻找核K，能够最大化地与标签集y对齐\\
max_{A,K} \langle K,yy^T\rangle\\
s.t. trace(A)\leq1\\
\left(\begin{array}{cc}A\qquad K^T\\
K\qquad I\end{array}\right) \geq0\\
K\geq0\\
考虑训练集S_{n_{tr}} = \{(x_1,y_1),..,(x_{n_{tr}},y_{n_{tr}})\}\\
测试集T_{n_{t}}\{x_{n_{tr}+1},...,x_{n_{tr}+n_t}\}\\
则核具有如下形式：\\
K = \left(\begin{array}{cc}K_{tr}\qquad K_{trt}\\
K^T_{trt}\qquad K_t\end{array}\right)\\
K_{ij}=\langle \phi(x_i) ,\phi(x_j)\rangle
$$

### 二次约束二次规划

-支持核机（SKM）
$$
将每个数据x_i分解成m个块，x_i= \{x_{1i},...,x_{mi}\}\\
目标是寻找一个线性分类器y = sign(w^Tx+b),w = w_1,...,w_m,\\
为了获得向量w地稀疏性，让w尽可能多地含有0，使用了L1，L2norm\\
优化问题如下：\\
min\frac{1}{2}(\sum_{j=1}^m d_j||w_j||_2)^2+C\sum_{i=1}^n\xi_i\\
w.r.t. (关于)\\
s.t, y_i(\sum_j w^T_jx_{ji}+b)\geq 1-\xi_i,\forall i\in\{1,...,n\}
$$
可以视作二阶锥规划

![image-20220809120013526](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220809120013526.png)

后续SMO算法（快速SVM）

![image-20220809120245056](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220809120245056.png)

### 半无限线性规划

![image-20220809153207805](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220809153207805.png)

### 简单MKL

![image-20220809153621429](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220809153621429.png)

转换成

![image-20220809153746726](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220809153746726.png)

![image-20220809153758079](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220809153758079.png)

首先解决给定d经典SVM优化问题J(d)，然后当通过梯度下降来更新d并保证对d的约束

### Group-LASSO法

当核可以被分作是输入或源的子集时候，可以考虑组合内核之间的组结构

在学习过程中要抑制与该任务无关的核

![image-20220809155526061](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220809155526061.png)

### 学习核的界限

当一个核是从k个基核的凸组合中选出的，那么评估的错误将会被限制在很小范围

## 基于子空间的方法

假设输入视图是从该子空间生成的，来获得由多个视图共享的潜在子空间。

### 基于CCA的算法

用于对两组（或更多）变量之间的关系进行建模的技术

#### 回顾CCA

上面有了

#### 核CCA

核提供了解决非线性的问题，通过将数据铺平到高维空间，然后使用线性方法

给定数据集X,Y,CCA寻找线性预测wx核wy，通过预测，两个数据集中的对应示例在投影空间中具有最大相关性，为了获得CCA的核公式，通过将投影方向变成对偶表示。wx = Xα，wy = Yβ，这两个稀疏都是N维向量，在对偶中，XY的相关系数可以表示为
$$
\rho = \frac{\alpha^T X^TXY^TY\beta}{\sqrt{(\alpha^T X^TXX^TX\alpha)(\beta^TY^TYY^TY\beta)}}\\
当K_x = X^TX,K_y = Y^TY\\
核CCA就是去解决如下问题：\\
max_{\alpha,\beta}  = \frac{\alpha^TK_xK_y\beta}{\sqrt{\alpha^TK_x^2\alpha×\beta^TK_y^2\beta}}\\
s.t. \alpha^TK_x^2\alpha = 1,\beta^TK_y^2\beta = 1
$$
核就是Kx和Ky，就是两个视角上的。

与通过协方差矩阵进行特征分解的线性CCA不同，KCCA问题是如下：

![image-20220811101851129](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220811101851129.png)

KCCA 可能是提高分类算法（如支持向量机）性能的有效预处理步骤

与表征 KCCA 的 2 范数略有不同，SVM-2K 采用 ε 不敏感的 1 范数，使用松弛变量来测量点未能满足 ε 相似性的数量：

![image-20220811111020628](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220811111020628.png)

w和b都是第一步和第二部SVM各自的权重和偏置值，然后使用SVM约束的1范数，优化问题如下：

![image-20220811112110427](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220811112110427.png)

最终决定的函数是：
$$
f(x) = \frac{1}{2}(f_A(x)+f_B(x))
$$

#### CCA的理论分析

对两组变量寻找基向量，使得投影到这些基向量![image-20220813104231176](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220813104231176.png)

上的相关性相互最大化

KCCA是使用核方法的非线性CCA的版本，通过寻找f∈Hx核g∈Hy，让随机变量fx和gy有最大相关性

所以KCCA的公式

![image-20220813113649143](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220813113649143.png)

事实上，我们必须从优先样本中估计所需要函数，所以经验估计是

![image-20220813113748089](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220813113748089.png)

\epsilon是规则化系数，n是样本数量，

通过提供正则化参数的比例，来研究建立KCCA一致性的一般问题，并证明当

![image-20220813143356536](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220813143356536.png)

保证了CCA的收敛

-使用回归公式对KCCA进行有效样本分析

通过计算经验期望值![image-20220813150651115](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220813150651115.png)![image-20220813150659576](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220813150659576.png)

新数据的错误界限可以通过使用Rademach复杂计算

#### CCA相关算法

-监督学习算法

一个视角是从数据派生的，另一个视角是从类标签派生的。

在这个数据集中，数据可以被投影到由标签信息引导的低维空间，但这是单视角

-Generalized Multi-view Analysis

流行的有监督和无监督特征提取技术是一种特殊形式的二次约束二次规划的解决方案这一事实

将数据投影到所跨越的子空间，然后将标准聚类算法应用于该子空间。

-半监督学习算法

提出了一种方法，该方法可以仅使用一个标记的训练示例执行半监督学习

可以度量原始无标记样本和原始标记样本之间的相似性。因此，可以分别选择具有最高和最低相似度分数的几个未标记示例作为额外的正例和负例。随着标记训练样例数量的增加，可以执行传统的半监督学习算法

### Multi-view Fisher Discriminant Analysis  （LDA的多视角模型）

概括了 Fisher 的判别分析，以在有监督的环境中找到多视图数据的信息投影。

#### 二视角Fisher Discriminant Analysis

同一对象的两个视图的数据Xa和Xb，选择两对权重wa和wb去解决如下优化问题

![image-20220814100318734](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220814100318734.png)

wa和wb是权重向量

优化约束条件是这个 

![image-20220814104622077](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220814104622077.png)

后续可以写成拉格朗日形式

![image-20220814104646702](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220814104646702.png)

#### 核二视角Fisher Discriminant Analysis

通过对偶权重向量
$$
w_a = X^T_a\alpha和w_b = X^T_b\beta
$$
可以得到对偶形式

![image-20220814111520893](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220814111520893.png)

核形式

![image-20220814111534812](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220814111534812.png)

约束条件为

![image-20220814111643507](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220814111643507.png)

用拉格朗日乘子法

![image-20220814111703653](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220814111703653.png)

### 多视角嵌入

#### MSE 多视角结构嵌入

### 多视角度量学习

从不同表示的数据中构建嵌入投影到一个共享的特征空间中，使得该空间中的欧几里得距离不仅在单个视图内有意义，而且在不同视图之间也有意义。



度量w1和w2是学习的目标

![image-20220814155449167](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220814155449167.png)

L是损失函数，Ω是参数γ和η的标准化

#### SSM-DML半监督的多视角度量学习

构建一个准确的度量来精确测量与多个视图相关的不同示例之间的差异。

定义F是xi对于标签yi的置信度，F矩阵可以通过如下函数获得

![image-20220814161434321](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220814161434321.png)

W是仿射矩阵

表示xi和xj之间的相异性度量，D是对角矩阵，Dii等于W的第i行的和

公式第一部分表示图上标签的平滑度，第二项表示训练数据的约束。假设 Xi 代表示例的第 i 个视图；

通过权重α线性组合由多视图特征集构建的图，等式可以扩展到多视图特征集

![image-20220814172826716](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220814172826716.png)

然后通过采用交替优化来解决上述优化问题，SSM-DML 可以同时从多个特征集和未标记数据的标签中学习多视图距离度量。



### 隐空间模型

可以通过隐变量，不同的视角将会被连接，信息可以从一个视角传到另一个视角

#### 共享高斯过程隐变量模型SGPLV

-高斯过程隐变量模型

一种非线性维数减少模型

让Y，Z作为从DY和DZ空间中提取的观测矩阵，X是隐空间维度。假设每个隐点xi生成一堆观察yi和zi，通过高斯过程参数化非线性函数fy和fz，通过使用指数核去定义两个数据点之间的相似性

![image-20220815103947372](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220815103947372.png)

![image-20220815104808066](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220815104808066.png)

通过使用共轭梯度解决最大化公式62，模型可以学习每个观察空间的核以及一组共同的潜在点。

-给定一个经过训练的 SGPLVM，我们希望在给定另一个观察空间中的参数的情况下推断一个观察空间中的参数

两步：

首先使用maxLx（x，y）去确定给定观测值y的最可能潜在坐标x

一旦为给定的 y 推断出正确的潜在坐标 x，该模型就会使用经过训练的 SGPL VM 来预测相应的观察值 z。

#### 共享核信息嵌入

给定样本服从分布p（x），核信息嵌入目的是寻找一个低维隐分布p（z），可以解析数据的结构，以及潜在空间和数据空间之间的显式双向概率映射。

KIE 寻找连接分布p（x，z）可以最大化潜在分布和数据分布之间的互信息

![image-20220815120908597](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220815120908597.png)

H是香农熵，可以通过核密度估计

而共享就是可以在通过最大化互信息 I((x, y), z) 构造两个视图的联合嵌入。

![image-20220815121152298](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220815121152298.png)

人体姿态检测

#### 正交分解隐空间FOLS

前两种方法只考虑数据的互信息，但忽略了每个视角各自的部分

通过引入正交约束将潜在空间分解为共享空间和私有空间，这会惩罚冗余的潜在表示。

对于最小分解，共享和私有潜在空间需要是非冗余的；换句话说，希望惩罚不同私人空间的冗余，从而鼓励共享空间中公共信息的表示。

Y是来自单个视图i的一组观察值，X是各个视角的隐空间，Z是i个视角的私人空间，M是每个视角相交的私人-共享空间。m = 【x，z】，FOLS模型就是最小化如下

![image-20220815142705674](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220815142705674.png)

si是M的奇异值，E0是i的能量，L是损失函数

#### 具有结构化稀疏性的因子化潜在空间

该算法将每个视图表示为依赖于视图的字典条目的线性组合

虽然字典特定于每个视图，但这些字典的权重充当潜在变量并且对于所有视图都是相同的

要找到表示多个输入模式的潜在嵌入 α 的共享-私有分解，并且目标是找到一个字典D = {D1,...,DV}

![image-20220815143742785](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220815143742785.png)

第一项衡量损失，第二项鼓励每个视图仅使用有限数量的潜在维度，第三项表示放松等级约束以发现潜在空间的维度。

给定一个观察集{x1，...，xv}对应的嵌入α可以通过如下凸优化问题解决得出

![image-20220815143926262](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220815143926262.png)

#### 隐空间马尔可夫网

基于通用多视图潜在空间马尔可夫网络构建了多视图数据共享的预测子空间

假设：假设在给定一组潜在变量的情况下，来自不同视图的数据和因变量是条件独立的。

双视图潜在空间马尔可夫网络由输入数据的两个视图组成X{Xn}，Z{Zm}，以及一系列潜在变量H：{Hk}

根据随机场理论吗，两个视图的边界分布分别可以写成指数形式

![image-20220815145137775](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220815145137775.png)

两个函数是特征函数，A和B是log函数，对于隐变量，边界分布可以是

![image-20220815145227163](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220815145227163.png)

通过在对数域中组合上述组件，联合模型分布定义为

![image-20220815145522771](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220815145522771.png)
