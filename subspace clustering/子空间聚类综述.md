# 子空间聚类综述

## 子空间聚类算法

一个子空间可以被表示为
$$
S_i = \{x \in R^D:x=\mu_i + U_iy\},i=1,...,n\\
\mu_i是子空间的任意点，如果等于0就是线性子空间，U_i是子空间的基，y是x的低维表示
$$


### 代数算法

用于聚类多个来自线性子空间的无噪声数据

#### 基于线性代数（矩阵分解）：

​	对于独立子空间具有正确性

​	这些算法从数据矩阵的低秩分解中获得数据分割
$$
设X_i \in R^{D × N_i} 是一个在子空间i中包含N_i个点得矩阵\\
数据矩阵的列可以根据n个子空间进行排序 [X_1,X_2,...,X_n] = X\Gamma\\
\Gamma \in R^{N ×N}是位置的参数矩阵\\
因为每个矩阵秩为d的X都可以被分解成：X_i = U_iY_i (正交分解）\qquad i=1,...,n\\
U_i \in R^{D×d_i}是子空间的正交基，Y_i \in R^{d_i ×N_i}是低秩表达\\
所以X\Gamma = UY
$$
​	那么子空间聚类问题就相当于是寻找一个参数矩阵\Gamma，可以通过SVD或者行列标准型进行计算，SVD中，ATA是表示各特征间关系的矩阵，所以V就是以数据集空间展开，AAT是表述样本数据间相关关系的矩阵，所以U就是以样本空间展开，步骤如下：
$$
让X = U\sum V^T,是经过SVD后的矩阵，\\
Q = VV^T \in R^{N×N}\\
在Q_{jk} = 0的地方就是j和k是不同的子空间\\
然后就对Q的的特征向量做谱聚类或者阈值化就行
$$

#### 基于多项式代数：

​	对于独立和非独立都有正确性

​	广义PCA(多项式法）：

​	一般主成分分析的主要思想是，可以用一组n次多项式拟合n个子空间的并集，其在一点上的导数给出一个向量，该向量垂直于包含该点的子空间

​	第一步就是将数据点投影到维度为r=dmax+1的子空间上，dmax是d1-dn的最大值，当子空间维度未知的时候，r是由所选择的模型决定的。

​	通过这一步，子空间聚类问题被简化为最多为dmax维数的聚类子空间

​	第二步就是拟合数据度数的多项式
$$
两个平面的并集是p(x) = (b_1^Tx)(b_2^Tx)=0,展开成二阶多项式：\\
c_1x_1^2 + c_2x_1x_2 + c_3x_1x_3+c_4x_2^2+c_5x_2x_3+c_6x_3^2=0\\
b_i是正交于平面的向量，c是系数\\
可以表示为c^Tv_n(x)，v_n(x)是所有在x中度为n的单项式向量\\
子空间数量n和维度d之间的关系\\
给定一个d，n=min\{i:rank(V_i)=M_i(r)-1\}
$$
在有噪声的情况下也可以通过齐次坐标应用在仿射空间上

可以使用最小二乘法-->假设向量C是Vn的左奇异值向量，对应最小的奇异值

​	最后一步就是计算系数向量c的法向量，这可以通过在数据点求多项式的导数得到

​		假设有两个子空间，则对并集求导=（b2Tx）b1 + （b1Tx）b2，如果x属于第一个子空间，就导数~b1，这为Si的正交补提供了一个基础，从中我们可以获得Si的基Ui。选择点的一种简单方法是选择任何数据点作为y1，以获得第一个子空间S1的基U1。在从数据集中删除属于S1的点后，我们可以选择任何剩余的数据点作为y2，以获得U2，从而得到S2，然后重复此过程，直到找到所有子空间。

#### 迭代法

​	给定一个初试的划分，可以使用传统PCA不断适应子空间，然后对每个子空间给一个PCA模型，这样就可以计算每个点离他最近的子空间。然后不断迭代。代表--kmeans



### 统计方法

#### 混合概率PCA

就是把数据看成是由概率向量组成的，降维就是求隐变量，其中求解方法用到了极大似然估计

#### agglomerative lossy compression

不像PPCA一样需要极大似然估计来确定模型参数，相反，它寻找的是数据分割，以最小化用退化高斯混合拟合点所需的编码长度，达到给定失真

步骤：

一开始每个数据点都作为一个分割的组，在每一次迭代过程中，如果这样做会最大程度减少编码长度就将两个组合并，直到不能再合并了为止。

#### 随机采样一致性算法

RANSAC算法是一种通过对观测数据进行随机抽样来估计模型参数的学习技术。对于给定的既包含内点又包含外点的数据集，RANSAC使用了投票方法来寻找最佳拟合结果。数据集中的数据元素用于为一个或多个模型投票。该投票方案的实施基于这样的两个假设：那些noisy 特征不会一致地全部投给任何单个模型；有足够多的特征可以就某一个好的模型达成一致。

RANSAC算法的输入是一组观测数据，一种将某种模型拟合到观测值的方法，以及一些置信参数。RANSAC通过重复以下步骤来实现目标：

1. 从原始数据集中随机采样出一个子集，把这个子集叫做hypothetical inliers(假设内点)；就是先假设我们随机一抽，抽出来的子集全部都为内点，都可以用模型来表达。

2. 用一个模型来拟合这些假设的内点。

3. 其它那些没有被随机采样到作为假设内点的数据用来测试步骤2中拟合好的模型，这些用来测试的数据中可以很好地符合步骤2中的估算模型的点被看作是共识集（consensus set）的一部分（是否很好地符合模型要根据模型特定的损失函数来判断）

4. 如果有足够多的点被归类为该共识集的一部分，那么说明这个模型很不错。

5. 之后，可以通过使用共识集的所有成员对模型进行重新估计进而改进模型。

这个过程重复固定的次数，每次产生一个模型，产生的这个模型，不是因为共识集的点太少而被拒绝，就是生成一个具有相应共识集大小的优化模型。



### 谱聚类方法

这类算法构建一个仿射矩阵，其矩阵的元素表示的是点j和k之间的相似度，Ajk=1表示在同一组，也可以是Ajk=exp（-disk^2）

给定仿射矩阵A，然后就可以从A形成的矩阵L的特征向量获得数据的分割，如果U是L的特征向量，则选择n<<N组成V矩阵。然后K均值算法就是用V

A如何形成L：可以选择L=A，L=diag（A1），标准拉普拉斯等

所以选择A很重要

#### 基于因式分解的相似性

同时基于矩阵分解和谱聚类

#### 基于GPCA的相似性

$$
A_{jk} = \prod_{m=1}^{min(d_j,d_k)}cos^2(\theta^m_{jk})\\
\theta 是两个面之间的夹角
$$

#### 局部子空间亲和性、谱局部适应平面

普遍现象：一个点和他最近的点经常会处在同一个子空间上

如果两个点j和k在同一个子平面上，他们的局部估计应当是一样的，如果是不一样的，就可以用局部估计来定义

第一步：使用X的奇异值分解将数据点投影到维数r=秩（X）的子空间

第二步：计算每个点j的k个邻居以此来是由局部仿射子空间     LSA的K由用户定并使用两个点的角度作为度量

第三步：计算仿射矩阵
$$
A_{jk} = exp[-\sum_{m=1}^{min(d_j.d_k)}sin^2(\theta^m_{jk})]
$$

#### 局部线性流形聚类

#### 稀疏子空间聚类

SSC使用稀疏性原则选择邻居

一个d维仿射或线性子空间的点可以被其他的d+1个同个子空间的点线性表示。

#### 低秩表达

#### 谱曲率聚类

基于多路聚类技术的线性流形聚类方法

考虑d+2个点，构建这些点属于同一子空间的可能性的度量，并使用该度量来构建两点之间的相似度

基于极曲率的概念定义相似性，当点位于同一子空间时，极曲率也为零

![image-20220731202648748](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220731202648748.png)
